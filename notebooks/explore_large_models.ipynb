{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-21 13:23:33.453672: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-21 13:23:33.555062: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-08-21 13:23:33.555080: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-08-21 13:23:34.207495: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-08-21 13:23:34.207551: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-08-21 13:23:34.207559: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: Unable to import optimizer(s) from expr_templates.py: No module named 'sympy'\n",
      "WARNING: Failed to import handlers from convolution.py: No module named 'torch'.\n",
      "WARNING: Failed to import handlers from core.py: No module named 'torch'.\n",
      "WARNING: Failed to import handlers from merge.py: No module named 'torch'.\n",
      "WARNING: Failed to import handlers from pooling.py: No module named 'torch'.\n",
      "WARNING: Failed to import handlers from reshape.py: No module named 'torch'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jcampos/miniforge3/envs/ml4qick-env/lib/python3.8/site-packages/hls4ml/converters/__init__.py:27: UserWarning: WARNING: Pytorch converter is not enabled!\n",
      "  warnings.warn(\"WARNING: Pytorch converter is not enabled!\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import sys \n",
    "sys.path.append(\"../training\")\n",
    "import pickle\n",
    "\n",
    "import hashlib\n",
    "import hls4ml \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import keras \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, BatchNormalization, Dropout, Softmax\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "from tensorflow_model_optimization.python.core.sparsity.keras import prune, pruning_callbacks, pruning_schedule\n",
    "from tensorflow_model_optimization.sparsity.keras import strip_pruning\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "from qkeras.qlayers import QDense, QActivation\n",
    "from qkeras import QBatchNormalization\n",
    "from qkeras.quantizers import quantized_bits, quantized_relu\n",
    "from qkeras.utils import _add_supported_quantized_objects\n",
    "from tensorflow.keras.models import load_model\n",
    "from qkeras.utils import _add_supported_quantized_objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "#### Impoartant! \n",
    "Download the dataset locally from [OneDrive here](https://purdue0-my.sharepoint.com/personal/du245_purdue_edu/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fdu245%5Fpurdue%5Fedu%2FDocuments%2FShared%2FQSC%20ML%20for%20readout%2FFinal%5Fraw%5Fdata%5Ffor%5Fpaper%2Fdata%5F0528%5Fnpy). We are using QICK data with timestamp **0528**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(data):\n",
    "    y_encoded = np.zeros([data.shape[0],2], dtype=np.int32)\n",
    "    for idx, x in enumerate(data):\n",
    "        if x == 1:\n",
    "            y_encoded[idx][1] = 1\n",
    "        else:\n",
    "            y_encoded[idx][0] = 1\n",
    "    return y_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Set:\n",
      "\tX Path        : ../data/malab_05282024/npz/0528_X_train_0_770.npy\n",
      "\ty Path        : ../data/malab_05282024/npz/0528_y_train_0_770.npy\n",
      "\tSize          : 900000\n",
      "\tSample Shape  : (1540,)\n",
      "\tMean          : 57.37779754545455\n",
      "\tStd. Dev.     : 844.0956096913322\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Loadning training split\"\"\"\n",
    "start_window = 0\n",
    "end_window = 770\n",
    "data_dir = \"../data/malab_05282024/npz/\"\n",
    "assert os.path.exists(f\"{data_dir}/0528_X_train_{start_window}_{end_window}.npy\"), \"File does not exist \"\n",
    "\n",
    "x_train_path = os.path.join(data_dir, f'0528_X_train_{start_window}_{end_window}.npy')\n",
    "y_train_path = os.path.join(data_dir, f'0528_y_train_{start_window}_{end_window}.npy')\n",
    "\n",
    "X_train_val = np.load(x_train_path)\n",
    "y_train_val = np.load(y_train_path)\n",
    "\n",
    "# Insure same dataset is loaded \n",
    "assert hashlib.md5(X_train_val).hexdigest() == 'b61226c86b7dee0201a9158455e08ffb',  \"Checksum failed. Wrong file was loaded or file may be corrupted.\"\n",
    "assert hashlib.md5(y_train_val).hexdigest() == 'c59ce37dc7c73d2d546e7ea180fa8d31',  \"Checksum failed. Wrong file was loaded or file may be corrupted.\"\n",
    "\n",
    "y_train_val = one_hot_encode(y_train_val)\n",
    "\n",
    "print(\"Train Data Set:\")\n",
    "print(\"\\tX Path        :\", x_train_path)\n",
    "print(\"\\ty Path        :\", y_train_path)\n",
    "print(\"\\tSize          :\", len(X_train_val))\n",
    "print(\"\\tSample Shape  :\", X_train_val[0].shape)\n",
    "print(\"\\tMean          :\", X_train_val.mean())\n",
    "print(\"\\tStd. Dev.     :\", X_train_val.std())\n",
    "\n",
    "assert len(X_train_val[0]) == (end_window-start_window)*2, \"ERROR: Specified window does not match loaded dataset shape\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data Set:\n",
      "\tX Path        : ../data/malab_05282024/npz/0528_X_test_0_770.npy\n",
      "\ty Path        : ../data/malab_05282024/npz/0528_y_test_0_770.npy\n",
      "\tSize         : 100000\n",
      "\tSample Shape : (1540,)\n",
      "\tSample Shape : 57.57549828571429\n",
      "\tStd. Dev.    : 845.6158899866076\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Loading testing split\"\"\"\n",
    "start_window = 0\n",
    "end_window = 770\n",
    "data_dir = \"../data/malab_05282024/npz/\"\n",
    "assert os.path.exists(f\"{data_dir}/X_test_{start_window}_{end_window}.npy\"), \"File does not exist \"\n",
    "\n",
    "x_test_path = os.path.join(data_dir, f'0528_X_test_{start_window}_{end_window}.npy')\n",
    "y_test_path = os.path.join(data_dir, f'0528_y_test_{start_window}_{end_window}.npy')\n",
    "\n",
    "X_test = np.load(x_test_path)\n",
    "y_test = np.load(y_test_path)\n",
    "\n",
    "# Insure same dataset is loaded \n",
    "assert hashlib.md5(X_test).hexdigest() == 'b7d85f42522a0a57e877422bc5947cde', \"Checksum failed. Wrong file was loaded or file may be corrupted.\"\n",
    "assert hashlib.md5(y_test).hexdigest() == '8c9cce1821372380371ade5f0ccfd4a2', \"Checksum failed. Wrong file was loaded or file may be corrupted.\"\n",
    "\n",
    "y_test = one_hot_encode(y_test)\n",
    "\n",
    "print(\"Test Data Set:\")\n",
    "print(\"\\tX Path        :\", x_test_path)\n",
    "print(\"\\ty Path        :\", y_test_path)\n",
    "print(\"\\tSize         :\", len(X_test))\n",
    "print(\"\\tSample Shape :\", X_test[0].shape)\n",
    "print(\"\\tSample Shape :\", X_test.mean())\n",
    "print(\"\\tStd. Dev.    :\", X_test.std())\n",
    "\n",
    "assert len(X_test[0]) == (end_window-start_window)*2, \"ERROR: Specified window does not match loaded dataset shape\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Multi-layer Model \n",
    "\n",
    "<!-- ![Multi-layer model](../images/multi_layer_model.png) -->\n",
    "<img src=\"../images/multi_layer_model.png\" alt=\"alt text\" width=\"75%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"hyperparameters\"\"\"\n",
    "init_learning_rate = 1e-4\n",
    "validation_split = 0\n",
    "batch_size = 8192\n",
    "epochs = 25\n",
    "checkpoint_filename = \"multi-layer.h5\"\n",
    "input_shape = (len(X_train_val[0]),)\n",
    "start_window = 0\n",
    "end_window = 770\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_37 (Dense)            (None, 385)               593285    \n",
      "                                                                 \n",
      " batch_normalization_24 (Bat  (None, 385)              1540      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 2)                 772       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 595,597\n",
      "Trainable params: 594,827\n",
      "Non-trainable params: 770\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "sr = int((end_window-start_window)*2)\n",
    "hn = sr * 2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(int(hn/8), activation='relu', input_shape=(sr,), kernel_regularizer=tf.keras.regularizers.L2(l2=1e-4),))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(2, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(l2=1e-4)))\n",
    "\n",
    "print(model.summary())\n",
    "assert model.count_params() == 595597, 'Error. Total parameters has changed.'\n",
    "\n",
    "baseline_param_count = model.count_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "        ModelCheckpoint(\n",
    "        checkpoint_filename,\n",
    "        monitor=\"val_loss\",\n",
    "        verbose=0,\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,\n",
    "        save_freq=\"epoch\",\n",
    "    ),\n",
    "    ReduceLROnPlateau(patience=5, min_delta=1**-6)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "105/105 [==============================] - 4s 32ms/step - loss: 0.2967 - accuracy: 0.9298 - val_loss: 0.2701 - val_accuracy: 0.9474 - lr: 1.0000e-04\n",
      "Epoch 2/25\n",
      "105/105 [==============================] - 3s 29ms/step - loss: 0.2341 - accuracy: 0.9574 - val_loss: 0.2396 - val_accuracy: 0.9536 - lr: 1.0000e-04\n",
      "Epoch 3/25\n",
      "105/105 [==============================] - 3s 29ms/step - loss: 0.2222 - accuracy: 0.9598 - val_loss: 0.2273 - val_accuracy: 0.9574 - lr: 1.0000e-04\n",
      "Epoch 4/25\n",
      "105/105 [==============================] - 3s 29ms/step - loss: 0.2138 - accuracy: 0.9612 - val_loss: 0.2199 - val_accuracy: 0.9590 - lr: 1.0000e-04\n",
      "Epoch 5/25\n",
      "105/105 [==============================] - 3s 29ms/step - loss: 0.2073 - accuracy: 0.9618 - val_loss: 0.2153 - val_accuracy: 0.9593 - lr: 1.0000e-04\n",
      "Epoch 6/25\n",
      "105/105 [==============================] - 3s 29ms/step - loss: 0.2018 - accuracy: 0.9623 - val_loss: 0.2120 - val_accuracy: 0.9597 - lr: 1.0000e-04\n",
      "Epoch 7/25\n",
      "105/105 [==============================] - 3s 29ms/step - loss: 0.1973 - accuracy: 0.9627 - val_loss: 0.2116 - val_accuracy: 0.9598 - lr: 1.0000e-05\n",
      "Epoch 8/25\n",
      "105/105 [==============================] - 3s 29ms/step - loss: 0.1968 - accuracy: 0.9628 - val_loss: 0.2115 - val_accuracy: 0.9598 - lr: 1.0000e-05\n",
      "Epoch 9/25\n",
      "105/105 [==============================] - 3s 29ms/step - loss: 0.1963 - accuracy: 0.9628 - val_loss: 0.2112 - val_accuracy: 0.9598 - lr: 1.0000e-05\n",
      "Epoch 10/25\n",
      "105/105 [==============================] - 3s 29ms/step - loss: 0.1958 - accuracy: 0.9628 - val_loss: 0.2110 - val_accuracy: 0.9598 - lr: 1.0000e-05\n",
      "Epoch 11/25\n",
      "105/105 [==============================] - 3s 29ms/step - loss: 0.1953 - accuracy: 0.9629 - val_loss: 0.2107 - val_accuracy: 0.9598 - lr: 1.0000e-05\n",
      "Epoch 12/25\n",
      "105/105 [==============================] - 3s 29ms/step - loss: 0.1947 - accuracy: 0.9629 - val_loss: 0.2108 - val_accuracy: 0.9598 - lr: 1.0000e-06\n",
      "Epoch 13/25\n",
      "105/105 [==============================] - 3s 29ms/step - loss: 0.1947 - accuracy: 0.9629 - val_loss: 0.2108 - val_accuracy: 0.9598 - lr: 1.0000e-06\n",
      "Epoch 14/25\n",
      "105/105 [==============================] - 3s 29ms/step - loss: 0.1946 - accuracy: 0.9629 - val_loss: 0.2107 - val_accuracy: 0.9598 - lr: 1.0000e-06\n",
      "Epoch 15/25\n",
      "105/105 [==============================] - 3s 29ms/step - loss: 0.1946 - accuracy: 0.9629 - val_loss: 0.2107 - val_accuracy: 0.9598 - lr: 1.0000e-06\n",
      "Epoch 16/25\n",
      "105/105 [==============================] - 3s 29ms/step - loss: 0.1945 - accuracy: 0.9629 - val_loss: 0.2107 - val_accuracy: 0.9598 - lr: 1.0000e-06\n",
      "Epoch 17/25\n",
      "105/105 [==============================] - 3s 29ms/step - loss: 0.1945 - accuracy: 0.9629 - val_loss: 0.2107 - val_accuracy: 0.9598 - lr: 1.0000e-07\n",
      "Epoch 18/25\n",
      "105/105 [==============================] - 3s 29ms/step - loss: 0.1944 - accuracy: 0.9629 - val_loss: 0.2107 - val_accuracy: 0.9598 - lr: 1.0000e-07\n",
      "Epoch 19/25\n",
      "105/105 [==============================] - 3s 29ms/step - loss: 0.1945 - accuracy: 0.9629 - val_loss: 0.2107 - val_accuracy: 0.9597 - lr: 1.0000e-07\n",
      "Epoch 20/25\n",
      "105/105 [==============================] - 3s 29ms/step - loss: 0.1945 - accuracy: 0.9629 - val_loss: 0.2107 - val_accuracy: 0.9598 - lr: 1.0000e-07\n",
      "Epoch 21/25\n",
      "105/105 [==============================] - 3s 29ms/step - loss: 0.1945 - accuracy: 0.9629 - val_loss: 0.2107 - val_accuracy: 0.9598 - lr: 1.0000e-07\n",
      "Epoch 22/25\n",
      "105/105 [==============================] - 3s 29ms/step - loss: 0.1944 - accuracy: 0.9629 - val_loss: 0.2107 - val_accuracy: 0.9598 - lr: 1.0000e-08\n",
      "Epoch 23/25\n",
      "105/105 [==============================] - 3s 29ms/step - loss: 0.1944 - accuracy: 0.9629 - val_loss: 0.2107 - val_accuracy: 0.9598 - lr: 1.0000e-08\n",
      "Epoch 24/25\n",
      "105/105 [==============================] - 3s 29ms/step - loss: 0.1944 - accuracy: 0.9629 - val_loss: 0.2107 - val_accuracy: 0.9598 - lr: 1.0000e-08\n",
      "Epoch 25/25\n",
      "105/105 [==============================] - 3s 28ms/step - loss: 0.1944 - accuracy: 0.9629 - val_loss: 0.2107 - val_accuracy: 0.9598 - lr: 1.0000e-08\n"
     ]
    }
   ],
   "source": [
    "opt = Adam(learning_rate=init_learning_rate)\n",
    "model.compile(\n",
    "    optimizer=opt, \n",
    "    loss=CategoricalCrossentropy(from_logits=True), \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_val, \n",
    "    y_train_val, \n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs, \n",
    "    validation_split=0.05,   # 45,000 samples\n",
    "    shuffle=True, \n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "co = {}\n",
    "_add_supported_quantized_objects(co)\n",
    "model = load_model(checkpoint_filename, custom_objects=co, compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125/3125 [==============================] - 4s 1ms/step\n",
      "Keras  Accuracy: 0.96094\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print(\"Keras  Accuracy: {}\".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ground and excited indices \n",
    "e_indices = np.where(np.argmax(y_test, axis=1) == 1)[0]\n",
    "g_indices = np.where(np.argmax(y_test, axis=1) == 0)[0]\n",
    "\n",
    "# separate ground and excited samples \n",
    "Xe_test = X_test[e_indices]\n",
    "ye_test = np.argmax(y_test, axis=1)[e_indices]\n",
    "\n",
    "Xg_test = X_test[g_indices]\n",
    "yg_test = np.argmax(y_test, axis=1)[g_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 3s 2ms/step\n",
      "Total correct: 47360\n",
      "Total incorrect: 2640\n",
      "Total samples: 50000\n",
      "Keras Excited Accuracy: 0.9472\n",
      "1563/1563 [==============================] - 3s 2ms/step\n",
      "Total correct: 48734\n",
      "Total incorrect: 1266\n",
      "Total samples: 50000\n",
      "Keras Ground Accuracy: 0.97468\n",
      "\n",
      "===================================\n",
      "Fidelity 0.92188\n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "# compute total correct for excited state \n",
    "ye_pred = model.predict(Xe_test)\n",
    "e_accuracy = accuracy_score(ye_test, np.argmax(ye_pred, axis=1))\n",
    "\n",
    "total_correct = (ye_test==np.argmax(ye_pred, axis=1)).astype(np.int8).sum()\n",
    "total_incorrect = (ye_test!=np.argmax(ye_pred, axis=1)).astype(np.int8).sum()\n",
    "\n",
    "print(\"Total correct:\", total_correct)\n",
    "print(\"Total incorrect:\", total_incorrect)\n",
    "print(\"Total samples:\", len(Xe_test) )\n",
    "print(\"Keras Excited Accuracy: {}\".format(e_accuracy))\n",
    "\n",
    "# compute total correct for ground state \n",
    "yg_pred = model.predict(Xg_test)\n",
    "g_accuracy = accuracy_score(yg_test, np.argmax(yg_pred, axis=1))\n",
    "\n",
    "total_correct = (yg_test==np.argmax(yg_pred, axis=1)).astype(np.int8).sum()\n",
    "total_incorrect = (yg_test!=np.argmax(yg_pred, axis=1)).astype(np.int8).sum()\n",
    "\n",
    "print(\"Total correct:\", total_correct)\n",
    "print(\"Total incorrect:\", total_incorrect)\n",
    "print(\"Total samples:\", len(Xg_test) )\n",
    "print(\"Keras Ground Accuracy: {}\".format(g_accuracy))\n",
    "\n",
    "# compute fidelity \n",
    "baseline_fidelity = 1-(1-g_accuracy)-(1-e_accuracy)\n",
    "print('\\n===================================')\n",
    "print('Fidelity', baseline_fidelity)\n",
    "print('===================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Larger Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fidelity(X_data, y_data):\n",
    "    # get ground and excited indices \n",
    "    e_indices = np.where(np.argmax(y_data, axis=1) == 1)[0]\n",
    "    g_indices = np.where(np.argmax(y_data, axis=1) == 0)[0]\n",
    "\n",
    "    # separate ground and excited samples \n",
    "    Xe_test = X_data[e_indices]\n",
    "    ye_test = np.argmax(y_data, axis=1)[e_indices]\n",
    "\n",
    "    Xg_test = X_data[g_indices]\n",
    "    yg_test = np.argmax(y_data, axis=1)[g_indices]\n",
    "\n",
    "    # compute total correct for excited state \n",
    "    ye_pred = model.predict(Xe_test)\n",
    "    e_accuracy = accuracy_score(ye_test, np.argmax(ye_pred, axis=1))\n",
    "\n",
    "    total_correct = (ye_test==np.argmax(ye_pred, axis=1)).astype(np.int8).sum()\n",
    "    total_incorrect = (ye_test!=np.argmax(ye_pred, axis=1)).astype(np.int8).sum()\n",
    "\n",
    "    print(\"Total correct:\", total_correct)\n",
    "    print(\"Total incorrect:\", total_incorrect)\n",
    "    print(\"Total samples:\", len(Xe_test) )\n",
    "    print(\"Keras Excited Accuracy: {}\".format(e_accuracy))\n",
    "\n",
    "    # compute total correct for ground state \n",
    "    yg_pred = model.predict(Xg_test)\n",
    "    g_accuracy = accuracy_score(yg_test, np.argmax(yg_pred, axis=1))\n",
    "\n",
    "    total_correct = (yg_test==np.argmax(yg_pred, axis=1)).astype(np.int8).sum()\n",
    "    total_incorrect = (yg_test!=np.argmax(yg_pred, axis=1)).astype(np.int8).sum()\n",
    "\n",
    "    print(\"Total correct:\", total_correct)\n",
    "    print(\"Total incorrect:\", total_incorrect)\n",
    "    print(\"Total samples:\", len(Xg_test) )\n",
    "    print(\"Keras Ground Accuracy: {}\".format(g_accuracy))\n",
    "\n",
    "    # compute fidelity \n",
    "    fidelity = 1-(1-g_accuracy)-(1-e_accuracy)\n",
    "    # print('\\n===================================')\n",
    "    # print('Fidelity', fidelity)\n",
    "    \n",
    "    # print('===================================')\n",
    "    return fidelity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"hyperparameters\"\"\"\n",
    "init_learning_rate = 1e-3\n",
    "validation_split = 0\n",
    "batch_size = 8192\n",
    "epochs = 10\n",
    "checkpoint_filename = \"temp.h5\"\n",
    "input_shape = (len(X_train_val[0]),)\n",
    "start_window = 0\n",
    "end_window = 770\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #1 \n",
    "- Increased neurons to first dense layer, added extra hidden layer. \n",
    "- Total parameter count = 3,570,492 (about 6x more than baseline)\n",
    "\n",
    "<img src=\"../images/explore_model_1.png\" alt=\"alt text\" width=\"75%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_62 (Dense)            (None, 1540)              2373140   \n",
      "                                                                 \n",
      " batch_normalization_39 (Bat  (None, 1540)             6160      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_63 (Dense)            (None, 770)               1186570   \n",
      "                                                                 \n",
      " batch_normalization_40 (Bat  (None, 770)              3080      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_64 (Dense)            (None, 2)                 1542      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,570,492\n",
      "Trainable params: 3,565,872\n",
      "Non-trainable params: 4,620\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Build model\"\"\"\n",
    "sr = int((end_window-start_window)*2)\n",
    "hn = sr * 2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(\n",
    "    Dense(int(hn/2), \n",
    "        activation='relu', \n",
    "        kernel_regularizer=tf.keras.regularizers.L2(l2=1e-4),\n",
    "        input_shape=(sr,)\n",
    "    )\n",
    ")\n",
    "model.add(BatchNormalization())\n",
    "model.add(\n",
    "    Dense(int(hn/4),\n",
    "        activation='relu',\n",
    "        kernel_regularizer=tf.keras.regularizers.L2(l2=1e-4)\n",
    "    )\n",
    ")\n",
    "model.add(BatchNormalization())\n",
    "model.add(\n",
    "    Dense(2, \n",
    "        activation='relu',\n",
    "        kernel_regularizer=tf.keras.regularizers.L2(l2=1e-4)\n",
    "        )\n",
    ")\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "105/105 [==============================] - 18s 159ms/step - loss: 0.4073 - accuracy: 0.9560 - val_loss: 0.3929 - val_accuracy: 0.9564 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "105/105 [==============================] - 17s 159ms/step - loss: 0.2997 - accuracy: 0.9643 - val_loss: 0.2823 - val_accuracy: 0.9602 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "105/105 [==============================] - 17s 160ms/step - loss: 0.2370 - accuracy: 0.9653 - val_loss: 0.2520 - val_accuracy: 0.9598 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "105/105 [==============================] - 17s 158ms/step - loss: 0.1991 - accuracy: 0.9661 - val_loss: 0.2551 - val_accuracy: 0.9602 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "105/105 [==============================] - 17s 159ms/step - loss: 0.1687 - accuracy: 0.9685 - val_loss: 0.2851 - val_accuracy: 0.9595 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "105/105 [==============================] - 17s 159ms/step - loss: 0.1549 - accuracy: 0.9708 - val_loss: 0.3079 - val_accuracy: 0.9592 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "105/105 [==============================] - 17s 159ms/step - loss: 0.1247 - accuracy: 0.9758 - val_loss: 0.3258 - val_accuracy: 0.9603 - lr: 1.0000e-04\n",
      "Epoch 8/10\n",
      "105/105 [==============================] - 17s 162ms/step - loss: 0.1073 - accuracy: 0.9786 - val_loss: 0.3455 - val_accuracy: 0.9601 - lr: 1.0000e-04\n",
      "Epoch 9/10\n",
      "105/105 [==============================] - 17s 161ms/step - loss: 0.0996 - accuracy: 0.9793 - val_loss: 0.3640 - val_accuracy: 0.9601 - lr: 1.0000e-04\n",
      "Epoch 10/10\n",
      "105/105 [==============================] - 17s 159ms/step - loss: 0.0945 - accuracy: 0.9802 - val_loss: 0.3724 - val_accuracy: 0.9597 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Train\"\"\"\n",
    "checkpoint_filename = 'explore_model_1.h5'\n",
    "callbacks = [\n",
    "        ModelCheckpoint(\n",
    "        checkpoint_filename,\n",
    "        monitor=\"val_loss\",\n",
    "        verbose=0,\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,\n",
    "        save_freq=\"epoch\",\n",
    "    ),\n",
    "    ReduceLROnPlateau(patience=5, min_delta=1**-6)\n",
    "]\n",
    "\n",
    "opt = Adam(learning_rate=init_learning_rate)\n",
    "model.compile(\n",
    "    optimizer=opt, \n",
    "    loss=CategoricalCrossentropy(from_logits=True), \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_val, \n",
    "    y_train_val, \n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs, \n",
    "    validation_split=0.05,   # 45,000 samples\n",
    "    shuffle=True, \n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load checkpoint\"\"\"\n",
    "co = {}\n",
    "_add_supported_quantized_objects(co)\n",
    "model = load_model(checkpoint_filename, custom_objects=co, compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125/3125 [==============================] - 12s 4ms/step\n",
      "Test Accuracy: 0.96131\n",
      "1563/1563 [==============================] - 6s 4ms/step\n",
      "Total correct: 47258\n",
      "Total incorrect: 2742\n",
      "Total samples: 50000\n",
      "Keras Excited Accuracy: 0.94516\n",
      "1563/1563 [==============================] - 6s 4ms/step\n",
      "Total correct: 48873\n",
      "Total incorrect: 1127\n",
      "Total samples: 50000\n",
      "Keras Ground Accuracy: 0.97746\n",
      "Test Fidelity: 0.92262\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluate\"\"\"\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Test Accuracy: {}\".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1))))\n",
    "\n",
    "model_1_fidelity = compute_fidelity(X_data=X_test, y_data=y_test)\n",
    "print('Test Fidelity:', model_1_fidelity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #2\n",
    "- Increased neurons to first dense layer, added 2 extra hidden layer. \n",
    "- Total parameter count = 4,457,917 (about 7.5x more than baseline)\n",
    "\n",
    "<img src=\"../images/explore_model_2.png\" alt=\"alt text\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_65 (Dense)            (None, 1540)              2373140   \n",
      "                                                                 \n",
      " batch_normalization_41 (Bat  (None, 1540)             6160      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_66 (Dense)            (None, 770)               1186570   \n",
      "                                                                 \n",
      " batch_normalization_42 (Bat  (None, 770)              3080      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_67 (Dense)            (None, 770)               593670    \n",
      "                                                                 \n",
      " batch_normalization_43 (Bat  (None, 770)              3080      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_68 (Dense)            (None, 385)               296835    \n",
      "                                                                 \n",
      " batch_normalization_44 (Bat  (None, 385)              1540      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_69 (Dense)            (None, 2)                 772       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,464,847\n",
      "Trainable params: 4,457,917\n",
      "Non-trainable params: 6,930\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Build model\"\"\"\n",
    "sr = int((end_window-start_window)*2)\n",
    "hn = sr * 2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(\n",
    "    Dense(int(hn/2), \n",
    "        activation='relu',\n",
    "        input_shape=(sr,), \n",
    "        kernel_regularizer=tf.keras.regularizers.L2(l2=1e-4)\n",
    "))\n",
    "model.add(BatchNormalization())\n",
    "model.add(\n",
    "    Dense(int(hn/4), \n",
    "        activation='relu',\n",
    "        kernel_regularizer=tf.keras.regularizers.L2(l2=1e-4)\n",
    "))\n",
    "model.add(BatchNormalization())\n",
    "model.add(\n",
    "    Dense(int(hn/4), \n",
    "        activation='relu',\n",
    "        kernel_regularizer=tf.keras.regularizers.L2(l2=1e-4)\n",
    "))\n",
    "model.add(BatchNormalization())\n",
    "model.add(\n",
    "    Dense(int(hn/8), \n",
    "        activation='relu',\n",
    "        kernel_regularizer=tf.keras.regularizers.L2(l2=1e-4)\n",
    "))\n",
    "model.add(BatchNormalization())\n",
    "model.add(\n",
    "    Dense(2, \n",
    "        activation='relu',\n",
    "        kernel_regularizer=tf.keras.regularizers.L2(l2=1e-4)\n",
    "))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "105/105 [==============================] - 24s 215ms/step - loss: 0.5193 - accuracy: 0.9565 - val_loss: 0.5729 - val_accuracy: 0.8791 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "105/105 [==============================] - 23s 216ms/step - loss: 0.3846 - accuracy: 0.9639 - val_loss: 0.3447 - val_accuracy: 0.9606 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "105/105 [==============================] - 23s 216ms/step - loss: 0.2974 - accuracy: 0.9645 - val_loss: 0.2911 - val_accuracy: 0.9590 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "105/105 [==============================] - 23s 216ms/step - loss: 0.2474 - accuracy: 0.9650 - val_loss: 0.2660 - val_accuracy: 0.9589 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "105/105 [==============================] - 23s 218ms/step - loss: 0.2179 - accuracy: 0.9661 - val_loss: 0.2567 - val_accuracy: 0.9595 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "105/105 [==============================] - 22s 214ms/step - loss: 0.1981 - accuracy: 0.9688 - val_loss: 0.2837 - val_accuracy: 0.9594 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "105/105 [==============================] - 23s 215ms/step - loss: 0.1356 - accuracy: 0.9877 - val_loss: 0.3338 - val_accuracy: 0.9602 - lr: 1.0000e-04\n",
      "Epoch 8/10\n",
      "105/105 [==============================] - 23s 215ms/step - loss: 0.1068 - accuracy: 0.9965 - val_loss: 0.3622 - val_accuracy: 0.9597 - lr: 1.0000e-04\n",
      "Epoch 9/10\n",
      "105/105 [==============================] - 23s 215ms/step - loss: 0.0952 - accuracy: 0.9988 - val_loss: 0.3799 - val_accuracy: 0.9590 - lr: 1.0000e-04\n",
      "Epoch 10/10\n",
      "105/105 [==============================] - 22s 213ms/step - loss: 0.0891 - accuracy: 0.9994 - val_loss: 0.3938 - val_accuracy: 0.9580 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Train\"\"\"\n",
    "checkpoint_filename = 'explore_model_2.h5'\n",
    "callbacks = [\n",
    "        ModelCheckpoint(\n",
    "        checkpoint_filename,\n",
    "        monitor=\"val_loss\",\n",
    "        verbose=0,\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,\n",
    "        save_freq=\"epoch\",\n",
    "    ),\n",
    "    ReduceLROnPlateau(patience=5, min_delta=1**-6)\n",
    "]\n",
    "\n",
    "opt = Adam(learning_rate=init_learning_rate)\n",
    "model.compile(\n",
    "    optimizer=opt, \n",
    "    loss=CategoricalCrossentropy(from_logits=True), \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_val, \n",
    "    y_train_val, \n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs, \n",
    "    validation_split=0.05,   # 45,000 samples\n",
    "    shuffle=True, \n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load best checkpoint\"\"\"\n",
    "co = {}\n",
    "_add_supported_quantized_objects(co)\n",
    "model = load_model(checkpoint_filename, custom_objects=co, compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125/3125 [==============================] - 15s 5ms/step\n",
      "Test Accuracy: 0.96122\n",
      "1563/1563 [==============================] - 8s 5ms/step\n",
      "Total correct: 47294\n",
      "Total incorrect: 2706\n",
      "Total samples: 50000\n",
      "Keras Excited Accuracy: 0.94588\n",
      "1563/1563 [==============================] - 7s 5ms/step\n",
      "Total correct: 48828\n",
      "Total incorrect: 1172\n",
      "Total samples: 50000\n",
      "Keras Ground Accuracy: 0.97656\n",
      "Test Fidelity: 0.92244\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluate\"\"\"\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Test Accuracy: {}\".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1))))\n",
    "\n",
    "model_2_fidelity = compute_fidelity(X_data=X_test, y_data=y_test)\n",
    "print('Test Fidelity:', model_2_fidelity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4qick-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
