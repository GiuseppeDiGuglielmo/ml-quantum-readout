{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-20 12:34:40.194480: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-20 12:34:40.294617: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-08-20 12:34:40.294636: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-08-20 12:34:40.797065: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-08-20 12:34:40.797136: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-08-20 12:34:40.797143: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: Unable to import optimizer(s) from expr_templates.py: No module named 'sympy'\n",
      "WARNING: Failed to import handlers from convolution.py: No module named 'torch'.\n",
      "WARNING: Failed to import handlers from core.py: No module named 'torch'.\n",
      "WARNING: Failed to import handlers from merge.py: No module named 'torch'.\n",
      "WARNING: Failed to import handlers from pooling.py: No module named 'torch'.\n",
      "WARNING: Failed to import handlers from reshape.py: No module named 'torch'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jcampos/miniforge3/envs/ml4qick-env/lib/python3.8/site-packages/hls4ml/converters/__init__.py:27: UserWarning: WARNING: Pytorch converter is not enabled!\n",
      "  warnings.warn(\"WARNING: Pytorch converter is not enabled!\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import sys \n",
    "sys.path.append(\"../training\")\n",
    "import pickle\n",
    "\n",
    "import hashlib\n",
    "import hls4ml \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, BatchNormalization, Dropout, Softmax\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "from tensorflow_model_optimization.python.core.sparsity.keras import prune, pruning_callbacks, pruning_schedule\n",
    "from tensorflow_model_optimization.sparsity.keras import strip_pruning\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "from qkeras.qlayers import QDense, QActivation\n",
    "from qkeras import QBatchNormalization\n",
    "from qkeras.quantizers import quantized_bits, quantized_relu\n",
    "from qkeras.utils import _add_supported_quantized_objects\n",
    "from tensorflow.keras.models import load_model\n",
    "from qkeras.utils import _add_supported_quantized_objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "#### Impoartant! \n",
    "Download the dataset locally from [OneDrive here](https://purdue0-my.sharepoint.com/personal/du245_purdue_edu/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fdu245%5Fpurdue%5Fedu%2FDocuments%2FShared%2FQSC%20ML%20for%20readout%2FFinal%5Fraw%5Fdata%5Ffor%5Fpaper%2Fdata%5F0528%5Fnpy). We are using QICK data with timestamp **0528**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(data):\n",
    "    y_encoded = np.zeros([data.shape[0],2], dtype=np.int32)\n",
    "    for idx, x in enumerate(data):\n",
    "        if x == 1:\n",
    "            y_encoded[idx][1] = 1\n",
    "        else:\n",
    "            y_encoded[idx][0] = 1\n",
    "    return y_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Set:\n",
      "\tX Path        : ../data/malab_05282024/npz/0528_X_train_0_770.npy\n",
      "\ty Path        : ../data/malab_05282024/npz/0528_y_train_0_770.npy\n",
      "\tSize          : 900000\n",
      "\tSample Shape  : (1540,)\n",
      "\tMean          : 57.37779754545455\n",
      "\tStd. Dev.     : 844.0956096913322\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Loadning training split\"\"\"\n",
    "start_window = 0\n",
    "end_window = 770\n",
    "data_dir = \"../data/malab_05282024/npz/\"\n",
    "assert os.path.exists(f\"{data_dir}/0528_X_train_{start_window}_{end_window}.npy\"), \"File does not exist \"\n",
    "\n",
    "x_train_path = os.path.join(data_dir, f'0528_X_train_{start_window}_{end_window}.npy')\n",
    "y_train_path = os.path.join(data_dir, f'0528_y_train_{start_window}_{end_window}.npy')\n",
    "\n",
    "X_train_val = np.load(x_train_path)\n",
    "y_train_val = np.load(y_train_path)\n",
    "\n",
    "# Insure same dataset is loaded \n",
    "assert hashlib.md5(X_train_val).hexdigest() == 'b61226c86b7dee0201a9158455e08ffb',  \"Checksum failed. Wrong file was loaded or file may be corrupted.\"\n",
    "assert hashlib.md5(y_train_val).hexdigest() == 'c59ce37dc7c73d2d546e7ea180fa8d31',  \"Checksum failed. Wrong file was loaded or file may be corrupted.\"\n",
    "\n",
    "y_train_val = one_hot_encode(y_train_val)\n",
    "\n",
    "print(\"Train Data Set:\")\n",
    "print(\"\\tX Path        :\", x_train_path)\n",
    "print(\"\\ty Path        :\", y_train_path)\n",
    "print(\"\\tSize          :\", len(X_train_val))\n",
    "print(\"\\tSample Shape  :\", X_train_val[0].shape)\n",
    "print(\"\\tMean          :\", X_train_val.mean())\n",
    "print(\"\\tStd. Dev.     :\", X_train_val.std())\n",
    "\n",
    "assert len(X_train_val[0]) == (end_window-start_window)*2, \"ERROR: Specified window does not match loaded dataset shape\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data Set:\n",
      "\tX Path        : ../data/malab_05282024/npz/0528_X_test_0_770.npy\n",
      "\ty Path        : ../data/malab_05282024/npz/0528_y_test_0_770.npy\n",
      "\tSize         : 100000\n",
      "\tSample Shape : (1540,)\n",
      "\tSample Shape : 57.57549828571429\n",
      "\tStd. Dev.    : 845.6158899866076\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Loading testing split\"\"\"\n",
    "start_window = 0\n",
    "end_window = 770\n",
    "data_dir = \"../data/malab_05282024/npz/\"\n",
    "assert os.path.exists(f\"{data_dir}/X_test_{start_window}_{end_window}.npy\"), \"File does not exist \"\n",
    "\n",
    "x_test_path = os.path.join(data_dir, f'0528_X_test_{start_window}_{end_window}.npy')\n",
    "y_test_path = os.path.join(data_dir, f'0528_y_test_{start_window}_{end_window}.npy')\n",
    "\n",
    "X_test = np.load(x_test_path)\n",
    "y_test = np.load(y_test_path)\n",
    "\n",
    "# Insure same dataset is loaded \n",
    "assert hashlib.md5(X_test).hexdigest() == 'b7d85f42522a0a57e877422bc5947cde', \"Checksum failed. Wrong file was loaded or file may be corrupted.\"\n",
    "assert hashlib.md5(y_test).hexdigest() == '8c9cce1821372380371ade5f0ccfd4a2', \"Checksum failed. Wrong file was loaded or file may be corrupted.\"\n",
    "\n",
    "y_test = one_hot_encode(y_test)\n",
    "\n",
    "print(\"Test Data Set:\")\n",
    "print(\"\\tX Path        :\", x_test_path)\n",
    "print(\"\\ty Path        :\", y_test_path)\n",
    "print(\"\\tSize         :\", len(X_test))\n",
    "print(\"\\tSample Shape :\", X_test[0].shape)\n",
    "print(\"\\tSample Shape :\", X_test.mean())\n",
    "print(\"\\tStd. Dev.    :\", X_test.std())\n",
    "\n",
    "assert len(X_test[0]) == (end_window-start_window)*2, \"ERROR: Specified window does not match loaded dataset shape\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer Model \n",
    "Or the initial \"big\" model \n",
    "\n",
    "<!-- ![Multi-layer model](../images/multi_layer_model.png) -->\n",
    "<img src=\"../images/multi_layer_model.png\" alt=\"alt text\" width=\"75%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"hyperparameters\"\"\"\n",
    "init_learning_rate = 1e-4\n",
    "validation_split = 0\n",
    "batch_size = 8192\n",
    "epochs = 10\n",
    "checkpoint_filename = \"multi-layer.h5\"\n",
    "input_shape = (len(X_train_val[0]),)\n",
    "start_window = 0\n",
    "end_window = 770\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 385)               593285    \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 385)              1540      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 772       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 595,597\n",
      "Trainable params: 594,827\n",
      "Non-trainable params: 770\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-20 12:35:04.743309: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2024-08-20 12:35:04.743359: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2024-08-20 12:35:04.743398: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (correlator4.fnal.gov): /proc/driver/nvidia/version does not exist\n",
      "2024-08-20 12:35:04.743781: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "sr = int((end_window-start_window)*2)\n",
    "hn = sr * 2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(int(hn/8), activation='relu', input_shape=(sr,)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(2, activation='relu'))\n",
    "\n",
    "print(model.summary())\n",
    "assert model.count_params() == 595597, 'Error. Total parameters has changed.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "        ModelCheckpoint(\n",
    "        checkpoint_filename,\n",
    "        monitor=\"val_loss\",\n",
    "        verbose=0,\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,\n",
    "        save_freq=\"epoch\",\n",
    "    ),\n",
    "    ReduceLROnPlateau(patience=75, min_delta=1**-6),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "105/105 [==============================] - 4s 34ms/step - loss: 0.2375 - accuracy: 0.9279 - val_loss: 0.1970 - val_accuracy: 0.9513 - lr: 1.0000e-04\n",
      "Epoch 2/10\n",
      "105/105 [==============================] - 3s 30ms/step - loss: 0.1748 - accuracy: 0.9574 - val_loss: 0.1761 - val_accuracy: 0.9554 - lr: 1.0000e-04\n",
      "Epoch 3/10\n",
      "105/105 [==============================] - 3s 30ms/step - loss: 0.1644 - accuracy: 0.9598 - val_loss: 0.1681 - val_accuracy: 0.9575 - lr: 1.0000e-04\n",
      "Epoch 4/10\n",
      "105/105 [==============================] - 3s 30ms/step - loss: 0.1577 - accuracy: 0.9610 - val_loss: 0.1632 - val_accuracy: 0.9586 - lr: 1.0000e-04\n",
      "Epoch 5/10\n",
      "105/105 [==============================] - 3s 30ms/step - loss: 0.1527 - accuracy: 0.9616 - val_loss: 0.1602 - val_accuracy: 0.9591 - lr: 1.0000e-04\n",
      "Epoch 6/10\n",
      "105/105 [==============================] - 3s 30ms/step - loss: 0.1488 - accuracy: 0.9620 - val_loss: 0.1581 - val_accuracy: 0.9594 - lr: 1.0000e-04\n",
      "Epoch 7/10\n",
      "105/105 [==============================] - 3s 30ms/step - loss: 0.1456 - accuracy: 0.9624 - val_loss: 0.1569 - val_accuracy: 0.9595 - lr: 1.0000e-04\n",
      "Epoch 8/10\n",
      "105/105 [==============================] - 3s 30ms/step - loss: 0.1428 - accuracy: 0.9628 - val_loss: 0.1564 - val_accuracy: 0.9598 - lr: 1.0000e-04\n",
      "Epoch 9/10\n",
      "105/105 [==============================] - 3s 30ms/step - loss: 0.1402 - accuracy: 0.9631 - val_loss: 0.1559 - val_accuracy: 0.9599 - lr: 1.0000e-04\n",
      "Epoch 10/10\n",
      "105/105 [==============================] - 3s 29ms/step - loss: 0.1377 - accuracy: 0.9635 - val_loss: 0.1559 - val_accuracy: 0.9600 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "opt = Adam(learning_rate=init_learning_rate)\n",
    "model.compile(\n",
    "    optimizer=opt, \n",
    "    loss=CategoricalCrossentropy(from_logits=True), \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_val, \n",
    "    y_train_val, \n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs, \n",
    "    validation_split=0.05, \n",
    "    shuffle=True, \n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125/3125 [==============================] - 8s 2ms/step\n",
      "Keras  Accuracy: 0.96107\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print(\"Keras  Accuracy: {}\".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ground and excited indices \n",
    "e_indices = np.where(np.argmax(y_test, axis=1) == 1)[0]\n",
    "g_indices = np.where(np.argmax(y_test, axis=1) == 0)[0]\n",
    "\n",
    "# separate ground and excited samples \n",
    "Xe_test = X_test[e_indices]\n",
    "ye_test = np.argmax(y_test, axis=1)[e_indices]\n",
    "\n",
    "Xg_test = X_test[g_indices]\n",
    "yg_test = np.argmax(y_test, axis=1)[g_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 4s 2ms/step\n",
      "Total correct: 47384\n",
      "Total incorrect: 2616\n",
      "Total samples: 50000\n",
      "Keras Excited Accuracy: 0.94768\n",
      "1563/1563 [==============================] - 4s 3ms/step\n",
      "Total correct: 48723\n",
      "Total incorrect: 1277\n",
      "Total samples: 50000\n",
      "Keras Ground Accuracy: 0.97446\n",
      "\n",
      "===================================\n",
      "Fidelity 0.96107\n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "# compute total correct for excited state \n",
    "ye_pred = model.predict(Xe_test)\n",
    "e_accuracy = accuracy_score(ye_test, np.argmax(ye_pred, axis=1))\n",
    "\n",
    "total_correct = (ye_test==np.argmax(ye_pred, axis=1)).astype(np.int8).sum()\n",
    "total_incorrect = (ye_test!=np.argmax(ye_pred, axis=1)).astype(np.int8).sum()\n",
    "\n",
    "print(\"Total correct:\", total_correct)\n",
    "print(\"Total incorrect:\", total_incorrect)\n",
    "print(\"Total samples:\", len(Xe_test) )\n",
    "print(\"Keras Excited Accuracy: {}\".format(e_accuracy))\n",
    "\n",
    "# compute total correct for ground state \n",
    "yg_pred = model.predict(Xg_test)\n",
    "g_accuracy = accuracy_score(yg_test, np.argmax(yg_pred, axis=1))\n",
    "\n",
    "total_correct = (yg_test==np.argmax(yg_pred, axis=1)).astype(np.int8).sum()\n",
    "total_incorrect = (yg_test!=np.argmax(yg_pred, axis=1)).astype(np.int8).sum()\n",
    "\n",
    "print(\"Total correct:\", total_correct)\n",
    "print(\"Total incorrect:\", total_incorrect)\n",
    "print(\"Total samples:\", len(Xg_test) )\n",
    "print(\"Keras Ground Accuracy: {}\".format(g_accuracy))\n",
    "\n",
    "# compute fidelity \n",
    "fidelity = 0.5*(e_accuracy + g_accuracy)\n",
    "print('\\n===================================')\n",
    "print('Fidelity', fidelity)\n",
    "print('===================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-layer Model \n",
    "Or the \"small\" model \n",
    "\n",
    "<img src=\"../images/single_layer_model.png\" alt=\"alt text\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"hyperparameters\"\"\"\n",
    "init_learning_rate = 1e-4\n",
    "validation_split = 0\n",
    "batch_size = 8192\n",
    "epochs = 10\n",
    "checkpoint_filename = \"single-layer.h5\"\n",
    "input_shape = (len(X_train_val[0]),)\n",
    "start_window = 0\n",
    "end_window = 770\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2 (Dense)             (None, 2)                 3082      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 2)                8         \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,090\n",
      "Trainable params: 3,086\n",
      "Non-trainable params: 4\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(2, activation='relu', input_shape=(sr,)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "print(model.summary())\n",
    "assert model.count_params() == 3090, 'Error. Total parameters has changed.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "        ModelCheckpoint(\n",
    "        checkpoint_filename,\n",
    "        monitor=\"val_loss\",\n",
    "        verbose=0,\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,\n",
    "        save_freq=\"epoch\",\n",
    "    ),\n",
    "    ReduceLROnPlateau(patience=75, min_delta=1**-6),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "105/105 [==============================] - 2s 16ms/step - loss: 0.6008 - accuracy: 0.6870 - val_loss: 0.3975 - val_accuracy: 0.8354 - lr: 1.0000e-04\n",
      "Epoch 2/10\n",
      "105/105 [==============================] - 1s 14ms/step - loss: 0.3645 - accuracy: 0.8877 - val_loss: 0.2941 - val_accuracy: 0.9169 - lr: 1.0000e-04\n",
      "Epoch 3/10\n",
      "105/105 [==============================] - 1s 14ms/step - loss: 0.3008 - accuracy: 0.9323 - val_loss: 0.2626 - val_accuracy: 0.9387 - lr: 1.0000e-04\n",
      "Epoch 4/10\n",
      "105/105 [==============================] - 1s 14ms/step - loss: 0.2718 - accuracy: 0.9443 - val_loss: 0.2477 - val_accuracy: 0.9458 - lr: 1.0000e-04\n",
      "Epoch 5/10\n",
      "105/105 [==============================] - 1s 14ms/step - loss: 0.2552 - accuracy: 0.9498 - val_loss: 0.2386 - val_accuracy: 0.9504 - lr: 1.0000e-04\n",
      "Epoch 6/10\n",
      "105/105 [==============================] - 1s 14ms/step - loss: 0.2443 - accuracy: 0.9529 - val_loss: 0.2320 - val_accuracy: 0.9520 - lr: 1.0000e-04\n",
      "Epoch 7/10\n",
      "105/105 [==============================] - 1s 14ms/step - loss: 0.2363 - accuracy: 0.9547 - val_loss: 0.2269 - val_accuracy: 0.9535 - lr: 1.0000e-04\n",
      "Epoch 8/10\n",
      "105/105 [==============================] - 1s 14ms/step - loss: 0.2302 - accuracy: 0.9558 - val_loss: 0.2227 - val_accuracy: 0.9545 - lr: 1.0000e-04\n",
      "Epoch 9/10\n",
      "105/105 [==============================] - 1s 14ms/step - loss: 0.2252 - accuracy: 0.9567 - val_loss: 0.2190 - val_accuracy: 0.9552 - lr: 1.0000e-04\n",
      "Epoch 10/10\n",
      "105/105 [==============================] - 1s 14ms/step - loss: 0.2209 - accuracy: 0.9572 - val_loss: 0.2158 - val_accuracy: 0.9556 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "opt = Adam(learning_rate=init_learning_rate)\n",
    "model.compile(\n",
    "    optimizer=opt, \n",
    "    loss=CategoricalCrossentropy(from_logits=True), \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_val, \n",
    "    y_train_val, \n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs, \n",
    "    validation_split=0.05, \n",
    "    shuffle=True, \n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125/3125 [==============================] - 5s 2ms/step\n",
      "Keras  Accuracy: 0.95684\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print(\"Keras  Accuracy: {}\".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ground and excited indices \n",
    "e_indices = np.where(np.argmax(y_test, axis=1) == 1)[0]\n",
    "g_indices = np.where(np.argmax(y_test, axis=1) == 0)[0]\n",
    "\n",
    "# separate ground and excited samples \n",
    "Xe_test = X_test[e_indices]\n",
    "ye_test = np.argmax(y_test, axis=1)[e_indices]\n",
    "\n",
    "Xg_test = X_test[g_indices]\n",
    "yg_test = np.argmax(y_test, axis=1)[g_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 3s 2ms/step\n",
      "Total correct: 46892\n",
      "Total incorrect: 3108\n",
      "Total samples: 50000\n",
      "Keras Excited Accuracy: 0.93784\n",
      "1563/1563 [==============================] - 2s 1ms/step\n",
      "Total correct: 48792\n",
      "Total incorrect: 1208\n",
      "Total samples: 50000\n",
      "Keras Ground Accuracy: 0.97584\n",
      "\n",
      "===================================\n",
      "Fidelity 0.95684\n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "# compute total correct for excited state \n",
    "ye_pred = model.predict(Xe_test)\n",
    "e_accuracy = accuracy_score(ye_test, np.argmax(ye_pred, axis=1))\n",
    "\n",
    "total_correct = (ye_test==np.argmax(ye_pred, axis=1)).astype(np.int8).sum()\n",
    "total_incorrect = (ye_test!=np.argmax(ye_pred, axis=1)).astype(np.int8).sum()\n",
    "\n",
    "print(\"Total correct:\", total_correct)\n",
    "print(\"Total incorrect:\", total_incorrect)\n",
    "print(\"Total samples:\", len(Xe_test) )\n",
    "print(\"Keras Excited Accuracy: {}\".format(e_accuracy))\n",
    "\n",
    "# compute total correct for ground state \n",
    "yg_pred = model.predict(Xg_test)\n",
    "g_accuracy = accuracy_score(yg_test, np.argmax(yg_pred, axis=1))\n",
    "\n",
    "total_correct = (yg_test==np.argmax(yg_pred, axis=1)).astype(np.int8).sum()\n",
    "total_incorrect = (yg_test!=np.argmax(yg_pred, axis=1)).astype(np.int8).sum()\n",
    "\n",
    "print(\"Total correct:\", total_correct)\n",
    "print(\"Total incorrect:\", total_incorrect)\n",
    "print(\"Total samples:\", len(Xg_test) )\n",
    "print(\"Keras Ground Accuracy: {}\".format(g_accuracy))\n",
    "\n",
    "# compute fidelity \n",
    "fidelity = 0.5*(e_accuracy + g_accuracy)\n",
    "print('\\n===================================')\n",
    "print('Fidelity', fidelity)\n",
    "print('===================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4qick-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
