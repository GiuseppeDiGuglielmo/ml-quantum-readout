{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable some console warnings\n",
    "import os\n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"../training\")\n",
    "import pickle\n",
    "\n",
    "from save_data import process_data\n",
    "\n",
    "import hls4ml \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, BatchNormalization, Dropout\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "from tensorflow_model_optimization.python.core.sparsity.keras import prune, pruning_callbacks, pruning_schedule\n",
    "from tensorflow_model_optimization.sparsity.keras import strip_pruning\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "from qkeras.qlayers import QDense, QActivation\n",
    "from qkeras import QBatchNormalization\n",
    "from qkeras.quantizers import quantized_bits, quantized_relu\n",
    "from qkeras.utils import _add_supported_quantized_objects\n",
    "from tensorflow.keras.models import load_model\n",
    "from qkeras.utils import _add_supported_quantized_objects\n",
    "\n",
    "# os.environ['PATH'] = os.environ['XILINX_VIVADO'] + '/bin:' + os.environ['PATH']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable/disable training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(data):\n",
    "    y_encoded = np.zeros([data.shape[0],2], dtype=np.int32)\n",
    "    for idx, x in enumerate(data):\n",
    "        if x == 1:\n",
    "            y_encoded[idx][1] = 1\n",
    "        else:\n",
    "            y_encoded[idx][0] = 1\n",
    "    return y_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_WINDOW = 285\n",
    "END_WINDOW = 385\n",
    "DATA_DIR = \"../data\"\n",
    "MODEL_DIR = \"models\"\n",
    "\n",
    "# convert raw ADC data into npy files \n",
    "if os.path.exists(f\"{DATA_DIR}/X_train_{START_WINDOW}_{END_WINDOW}.npy\") == False:\n",
    "    process_data(\n",
    "        start_window=START_WINDOW, \n",
    "        end_window=END_WINDOW, \n",
    "        data_dir=DATA_DIR\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "X_train_val = np.load(os.path.join(DATA_DIR, f'X_train_{START_WINDOW}_{END_WINDOW}.npy'))\n",
    "X_test = np.load(os.path.join(DATA_DIR, f'X_test_{START_WINDOW}_{END_WINDOW}.npy'))    \n",
    "y_train_val = np.load(os.path.join(DATA_DIR, f'y_train_{START_WINDOW}_{END_WINDOW}.npy'))\n",
    "y_test = np.load(os.path.join(DATA_DIR, f'y_test_{START_WINDOW}_{END_WINDOW}.npy'))\n",
    "\n",
    "y_train_val = one_hot_encode(y_train_val)\n",
    "y_test = one_hot_encode(y_test)\n",
    "\n",
    "print(\"Training:\")\n",
    "print(\"\\tSize:\", len(X_train_val))\n",
    "print(\"\\tSample Shape:\", len(X_train_val[0]))\n",
    "print(\"\\tMean:\", X_train_val.mean())\n",
    "print(\"\\tStd. Dev.:\", X_train_val.std())\n",
    "\n",
    "print(\"Testing:\")\n",
    "print(\"\\tSize:\", len(X_test))\n",
    "print(\"\\tSample Shape:\", len(X_test[0]))\n",
    "print(\"\\tSample Shape:\", X_test.mean())\n",
    "print(\"\\tStd. Dev.:\", X_test.std())\n",
    "\n",
    "assert len(X_train_val[0]) == (END_WINDOW-START_WINDOW)*2, \"ERROR: Specified window does not match loaded dataset shape\"\n",
    "assert len(X_test[0]) == (END_WINDOW-START_WINDOW)*2, \"ERROR: Specified window does not match loaded dataset shape\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct a model \n",
    "QKeras is \"Quantized Keras\" for deep heterogeneous quantization of ML models. We're using QDense layer instead of Dense. We're also training with model sparsity, since QKeras layers are prunable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INIT_LEARNING_RATE = 1e-2\n",
    "VALIDATION_SPLIT = 0.3\n",
    "BATCH_SIZE = 12800\n",
    "EPOCHS = 50\n",
    "CHECKPOINT_FILENAME = MODEL_DIR + \"/qmodel.h5\"\n",
    "INPUT_SHAPE = (len(X_train_val[0]),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    input_size = (END_WINDOW-START_WINDOW)*2\n",
    "\n",
    "    # Define model with quantize layers \n",
    "    model = Sequential()\n",
    "    model.add(QDense(2, input_shape=(input_size,), name='fc1', kernel_quantizer=quantized_bits(6,0,alpha=1), bias_quantizer=quantized_bits(6,0,alpha=1),))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # adding pruning \n",
    "    pruning_params = {'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50, final_sparsity=0.80, begin_step=200, end_step=1000)}\n",
    "    model = prune.prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "    print('=============================Model Summary=============================')\n",
    "    print(model.summary())\n",
    "    print('=======================================================================')\n",
    "else:\n",
    "    print(\"Training is disabled, load model from file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        INIT_LEARNING_RATE,\n",
    "        decay_steps=100000,\n",
    "        decay_rate=0.96,\n",
    "        staircase=True\n",
    "    )\n",
    "\n",
    "    callbacks = [\n",
    "            ModelCheckpoint(\n",
    "            CHECKPOINT_FILENAME,\n",
    "            monitor=\"val_loss\",\n",
    "            verbose=0,\n",
    "            save_best_only=True,\n",
    "            save_weights_only=False,\n",
    "            save_freq=\"epoch\",\n",
    "        ),\n",
    "        ReduceLROnPlateau(patience=75, min_delta=1**-6),\n",
    "        pruning_callbacks.UpdatePruningStep(),\n",
    "    ]\n",
    "else:\n",
    "    print(\"Training is disabled, load model from file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    opt = Adam(learning_rate=INIT_LEARNING_RATE)\n",
    "    model.compile(\n",
    "        optimizer=opt, \n",
    "        # loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "        loss=CategoricalCrossentropy(from_logits=True), \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train_val, \n",
    "        y_train_val, \n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS, \n",
    "        validation_split=VALIDATION_SPLIT, \n",
    "        shuffle=True, \n",
    "        callbacks=callbacks\n",
    "    )\n",
    "else:\n",
    "    print(\"Training is disabled, load model from file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important: Pruning layers must be removed before saving to disk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    model = strip_pruning(model)\n",
    "    model.save(CHECKPOINT_FILENAME)\n",
    "\n",
    "    history_file = CHECKPOINT_FILENAME.replace(\".h5\", \"-history.pkl\")\n",
    "    with open(history_file, 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)\n",
    "\n",
    "    print(f'Saving history to: {history_file}')\n",
    "    print(f'Saved checkpoint to: {CHECKPOINT_FILENAME}')\n",
    "else:\n",
    "    print(\"Training is disabled, load model from file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check performance\n",
    "\n",
    "todo: add ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co = {}\n",
    "_add_supported_quantized_objects(co)\n",
    "model = load_model(CHECKPOINT_FILENAME, custom_objects=co, compile=False)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Keras  Accuracy: {}\".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run with the model and data on files, you should expect\n",
    "```\n",
    "Keras  Accuracy: 0.8576534653465346\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check sparsity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = len(model.layers)\n",
    "print(f'Number of layers: {num_layers}')\n",
    "\n",
    "\n",
    "for idx in range(num_layers):\n",
    "    w = model.layers[idx].weights[0].numpy()\n",
    "    h, b = np.histogram(w, bins=100)\n",
    "\n",
    "    # plot weight distribution\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    plt.bar(b[:-1], h, width=b[1] - b[0])\n",
    "    plt.semilogy()\n",
    "    plt.savefig(f'model-dist-idx{idx}.png')\n",
    "\n",
    "    print('% of zeros = {}'.format(np.sum(w == 0) / np.size(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the HLS model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../utils\")\n",
    "from config import print_dict\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from qkeras.utils import _add_supported_quantized_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint \n",
    "co = {}\n",
    "_add_supported_quantized_objects(co)\n",
    "model = load_model(CHECKPOINT_FILENAME, custom_objects=co, compile=False)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Re-evalulate \n",
    "y_keras = model.predict(X_test)\n",
    "print(f'Model acc: {accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_keras, axis=1))}')\n",
    "print(model.summary()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HLS configuration \n",
    "HLSConfig = {}\n",
    "HLSConfig['Model'] = {}\n",
    "HLSConfig['Model']['Precision'] = 'ap_fixed<16,6>'  # Default precision\n",
    "HLSConfig['Model']['ReuseFactor'] = 1  # fully parallelized \n",
    "\n",
    "HLSConfig['LayerName'] = {}\n",
    "for layer in ['fc1_input', 'fc1', 'fc1_linear', 'batch_normalization']:\n",
    "    HLSConfig['LayerName'][layer] = {}\n",
    "    HLSConfig['LayerName'][layer]['Precision'] = {}\n",
    "    HLSConfig['LayerName'][layer]['Trace'] = True\n",
    "\n",
    "# Input - ZCU216 uses 14-bit ADCS \n",
    "HLSConfig['LayerName']['fc1_input']['Precision'] = 'ap_fixed<14,14>' \n",
    "# Fc\n",
    "HLSConfig['LayerName']['fc1']['Precision']['result'] = 'ap_fixed<18,18>'\n",
    "HLSConfig['LayerName']['fc1']['accum_t'] = 'ap_fixed<18,18>'\n",
    "# Fc Linear\n",
    "HLSConfig['LayerName']['fc1_linear']['Precision']['result'] = 'ap_fixed<18,18>'\n",
    "# Batchnormalization\n",
    "HLSConfig['LayerName']['batch_normalization']['Precision']['scale'] = 'ap_fixed<20,3>'\n",
    "HLSConfig['LayerName']['batch_normalization']['Precision']['bias'] = 'ap_fixed<20,3>'\n",
    "HLSConfig['LayerName']['batch_normalization']['Precision']['result'] = 'ap_fixed<16,6>'\n",
    "\n",
    "print_dict(HLSConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OutputDir = 'hls4ml_prj/'\n",
    "XilinxPart = 'xczu49dr-ffvf1760-2-e'\n",
    "IOType = 'io_parallel'\n",
    "ClockPeriod = 3.225  # 3.225ns (307.2 MHz)\n",
    "HLSFig = OutputDir+'model.png'\n",
    "\n",
    "hls_model = hls4ml.converters.convert_from_keras_model(\n",
    "    model=model,\n",
    "    hls_config=HLSConfig,\n",
    "    output_dir=OutputDir,\n",
    "    part=XilinxPart,\n",
    "    io_type=IOType,\n",
    "    clock_period=ClockPeriod,\n",
    "    project_name=\"NN\"\n",
    ")\n",
    "\n",
    "print(f\"Creating hls4ml project directory {OutputDir}\")\n",
    "hls_model.compile()  # Must compile for C Sim. \n",
    "\n",
    "# Visualize model\n",
    "hls4ml.utils.plot_model(\n",
    "    hls_model, show_shapes=True, show_precision=True, to_file=HLSFig\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trace output \n",
    "y_hls = hls_model.predict(np.ascontiguousarray(X_test.astype(np.float32))) \n",
    "y_hls = np.argmax(y_hls, axis=1)\n",
    "\n",
    "print(f'Keras Acc: {accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_keras, axis=1))*100:.5}%')\n",
    "print(f'HLS Acc: {accuracy_score(np.argmax(y_test, axis=1), y_hls)*100:.5}:%')\n",
    "print(f'CKA: {accuracy_score(np.argmax(y_keras, axis=1), y_hls)*100:.5}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation plots (Keras vs HLS)\n",
    "Let's compare the output of the Qkeras and HLS model. If properly configured, the HLS activations will be aligned with the Qkeras model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, hls_trace = hls_model.trace(np.ascontiguousarray(X_test.astype(np.float32))) \n",
    "keras_trace = hls4ml.model.profiling.get_ymodel_keras(model, X_test.astype(np.float32)) \n",
    "\n",
    "print(f'HLS Keys: {hls_trace.keys()}')\n",
    "print(f'Keras Keys: {keras_trace.keys()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = ['fc1', 'batch_normalization']\n",
    "\n",
    "for idx, layer in enumerate(layers):\n",
    "    keras_layer, hls_layer = keras_trace[layer], hls_trace[layer]\n",
    "    try:\n",
    "        diff = np.average(np.abs(keras_layer - hls_layer ))\n",
    "        print(f'{layer}', '\\t\\t', diff)\n",
    "        \n",
    "        plt.figure(figsize=(7, 5))\n",
    "\n",
    "        plt.scatter(hls_layer.flatten(), keras_layer.flatten())\n",
    "        min_x = min(keras_layer.min(), hls_layer.min())\n",
    "        max_x = min(keras_layer.max(), hls_layer.max())\n",
    "\n",
    "        onnx_min, onnx_max = keras_layer.flatten().min(), keras_layer.flatten().max()\n",
    "        hls_min, hls_max = hls_layer.flatten().min(), hls_layer.flatten().max()\n",
    "        \n",
    "        print(f'hls/keras min: {hls_min}/{onnx_min}')\n",
    "        print(f'hls/keras max: {hls_max}/{onnx_max}')\n",
    "        \n",
    "        plt.plot([min_x, max_x], [min_x, max_x], c='red')\n",
    "        plt.axhline(min_x, c='red')\n",
    "        plt.axhline(max_x, c='red')\n",
    "\n",
    "        plt.title(f'(hls) {layer} -- (keras) {layer}')\n",
    "        plt.xlabel(f'hls4ml - [{hls_min:.3f},  {hls_max:.3f}]')\n",
    "        plt.ylabel(f'keras - [{onnx_min:.3f},  {onnx_max:.3f}]')\n",
    "        plt.yscale('linear')\n",
    "    except Exception as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthesize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_model.build(\n",
    "    csim=False,\n",
    "    synth=True,\n",
    "    cosim=False,\n",
    "    export=True,\n",
    "    vsynth=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hls4ml.report.read_vivado_report(f'{OutputDir}/hls4ml_prj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare QKeras, hls4ml, and board runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "from ctypes import *\n",
    "def to_float(i):\n",
    "    cp = pointer(c_int(i))\n",
    "    fp = cast(cp, POINTER(c_float))\n",
    "    return fp.contents.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### demo00/send_receive_pulse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV files from board runs\n",
    "\n",
    "# X\n",
    "df = pd.read_csv(DATA_DIR + '/fpga_testing/X_fpga.csv', header=None)\n",
    "X_test_board = df.values\n",
    "\n",
    "# y\n",
    "df = pd.read_csv(DATA_DIR + '/fpga_testing/y_fpga.csv', header=None)\n",
    "y_test_board = df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X -> QKeras -> y\n",
    "y_qkeras_pred = model.predict(X_test_board)\n",
    "\n",
    "# X -> HLS -> y (csim)\n",
    "y_hls_pred = hls_model.predict(np.ascontiguousarray(X_test_board.astype(np.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = y_test_board.shape[0]\n",
    "errors = 0\n",
    "\n",
    "# Count mismatches (QKeras != HLS)\n",
    "for i in range(test_size):\n",
    "    g_ref = to_float(y_test_board[i][0])\n",
    "    e_ref = to_float(y_test_board[i][1])\n",
    "\n",
    "    g_hls = y_hls_pred[i][0]\n",
    "    e_hls = y_hls_pred[i][1]\n",
    "\n",
    "    mismatch = (g_ref != g_hls) | (e_ref != e_hls)\n",
    "    print('board[{}] [{:.12f}, {:.12f}]'.format(i, g_ref, e_ref))\n",
    "    print('hls  [{}] [{:.12f}, {:.12f}] {:s}'.format(i, g_hls, e_hls, '***' if mismatch else ''))\n",
    "    print('')\n",
    "    if (mismatch):\n",
    "        errors = errors + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_rate = (errors * 100.) / test_size\n",
    "print('Errors: {:d}/{:d} ({:.2f}%)'.format(errors, test_size, error_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run with the model and **board** data on files, you should expect\n",
    "```\n",
    "Errors: 761/6235 (12.21%)\n",
    "```\n",
    "**ATTENTION:** We should expect _0_ errors (i.e. logits should match) because we are comparing board vs. hls implementations.\n",
    "\n",
    "Given these and following results, my current guess is a problem with timing/triggering. The NN reads 200 I/Q samples after a trigger event + some delay. The delay in some cases should be a little earlier or later than expected.\n",
    "\n",
    "```\n",
    "I, Q, I, Q, I, Q, I, Q, I, Q, I, Q, I, Q, I, Q, I, Q, I, Q, I, Q, I, Q, I, Q, I, Q, I, Q, ..., I, Q, I, Q, I, Q\n",
    "      ^\n",
    "      Trigger\n",
    "      <-------->\n",
    "         delay\n",
    "                  <--------------------------------- 200 samples -------------------------------------------->\n",
    "\n",
    "             <=================================== 200 samples ==========================================>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mock readout data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ground state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV files from board runs\n",
    "\n",
    "# X (ground)\n",
    "df = pd.read_csv(DATA_DIR + '/fpga_testing/malab_g_state_X_fpga.csv', header=None)\n",
    "X_test_board = df.values\n",
    "\n",
    "# y (ground)\n",
    "df = pd.read_csv(DATA_DIR + '/fpga_testing/malab_g_state_y_fpga.csv', header=None)\n",
    "y_test_board = df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X -> QKeras -> y\n",
    "y_qkeras_pred = model.predict(X_test_board)\n",
    "\n",
    "# X -> HLS -> y (csim)\n",
    "y_hls_pred = hls_model.predict(np.ascontiguousarray(X_test_board.astype(np.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = y_test_board.shape[0]\n",
    "\n",
    "logit_errors = 0\n",
    "class_errors = 0\n",
    "\n",
    "# Count mismatches (QKeras != HLS)\n",
    "# Count ground predictions (HLS)\n",
    "for i in range(test_size):\n",
    "    g_ref = to_float(y_test_board[i][0])\n",
    "    e_ref = to_float(y_test_board[i][1])\n",
    "\n",
    "    g_hls = y_hls_pred[i][0]\n",
    "    e_hls = y_hls_pred[i][1]\n",
    "\n",
    "    mismatch = (g_ref != g_hls) | (e_ref != e_hls)\n",
    "    ground_state_ref = (g_ref > e_ref)\n",
    "    ground_state_hls = (g_hls > e_hls)\n",
    "    print('board[{}] [{:.12f}, {:.12f}] {:s} '.format(i, g_ref, e_ref, 'G' if ground_state_ref else 'E'))\n",
    "    print('hls  [{}] [{:.12f}, {:.12f}] {:s} {:s}'.format(i, g_hls, e_hls, 'G' if ground_state_hls else 'E', '***' if mismatch else ''))\n",
    "    print('')\n",
    "    if (mismatch):\n",
    "        logit_errors = logit_errors + 1\n",
    "    if (not ground_state_ref):\n",
    "        class_errors = class_errors + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_error_rate = (logit_errors * 100.) / test_size\n",
    "class_error_rate = (class_errors * 100.) / test_size\n",
    "print('Logit errors: {:d}/{:d} ({:.2f}%)'.format(logit_errors, test_size, logit_error_rate))\n",
    "print('Class errors: {:d}/{:d} ({:.2f}%)'.format(class_errors, test_size, class_error_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run with the model and **board** data on files, you should expect\n",
    "```\n",
    "Logit errors: 435/3100 (14.03%)\n",
    "Class errors: 0/3100 (0.00%)\n",
    "```\n",
    "**ATTENTION:** We should expect _0_ errors because we are comparing board vs. hls implementations.\n",
    "\n",
    "Given these results, my current guess is a problem with timing/triggering. The NN reads 200 I/Q samples after a trigger event + some delay. The delay in some cases should be a little earlier or later than expected.\n",
    "\n",
    "```\n",
    "I, Q, I, Q, I, Q, I, Q, I, Q, I, Q, I, Q, I, Q, I, Q, I, Q, I, Q, I, Q, I, Q, I, Q, I, Q, ..., I, Q, I, Q, I, Q\n",
    "      ^\n",
    "      Trigger\n",
    "      <-------->\n",
    "         delay\n",
    "                  <--------------------------------- 200 samples -------------------------------------------->\n",
    "\n",
    "             <=================================== 200 samples ==========================================>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Excited state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV files from board runs\n",
    "\n",
    "# X (excited)\n",
    "df = pd.read_csv(DATA_DIR + '/fpga_testing/malab_e_state_X_fpga.csv', header=None)\n",
    "X_test_board = df.values\n",
    "\n",
    "# y (excited)\n",
    "df = pd.read_csv(DATA_DIR + '/fpga_testing/malab_e_state_y_fpga.csv', header=None)\n",
    "y_test_board = df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X -> QKeras -> y\n",
    "y_qkeras_pred = model.predict(X_test_board)\n",
    "\n",
    "# X -> HLS -> y (csim)\n",
    "y_hls_pred = hls_model.predict(np.ascontiguousarray(X_test_board.astype(np.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = y_test_board.shape[0]\n",
    "\n",
    "logit_errors = 0\n",
    "class_errors = 0\n",
    "\n",
    "# Count mismatches (QKeras != HLS)\n",
    "# Count excited predictions (HLS)\n",
    "for i in range(test_size):\n",
    "    g_ref = to_float(y_test_board[i][0])\n",
    "    e_ref = to_float(y_test_board[i][1])\n",
    "\n",
    "    g_hls = y_hls_pred[i][0]\n",
    "    e_hls = y_hls_pred[i][1]\n",
    "\n",
    "    mismatch = (g_ref != g_hls) | (e_ref != e_hls)\n",
    "    excited_state_ref = (e_ref > g_ref)\n",
    "    excited_state_hls = (e_hls > g_hls)\n",
    "\n",
    "    print('board[{}] [{:.12f}, {:.12f}] {:s} '.format(i, g_ref, e_ref, 'E' if excited_state_ref else 'G'))\n",
    "    print('hls  [{}] [{:.12f}, {:.12f}] {:s} {:s}'.format(i, g_hls, e_hls, 'E' if excited_state_hls else 'G', '***' if mismatch else ''))\n",
    "    print('')\n",
    "    if (mismatch):\n",
    "        logit_errors = logit_errors + 1\n",
    "    if (not excited_state_ref):\n",
    "        class_errors = class_errors + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_error_rate = (logit_errors * 100.) / test_size\n",
    "class_error_rate = (class_errors * 100.) / test_size\n",
    "print('Logit errors: {:d}/{:d} ({:.2f}%)'.format(logit_errors, test_size, logit_error_rate))\n",
    "print('Class errors: {:d}/{:d} ({:.2f}%)'.format(class_errors, test_size, class_error_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run with the model and **board** data on files, you should expect\n",
    "```\n",
    "Logit errors: 440/3100 (14.19%)\n",
    "Class errors: 0/3100 (0.00%)\n",
    "```\n",
    "**ATTENTION:** We should expect _0_ errors because we are comparing board vs. hls implementations.\n",
    "\n",
    "Given these results, my current guess is a problem with timing/triggering. The NN reads 200 I/Q samples after a trigger event + some delay. The delay in some cases should be a little earlier or later than expected.\n",
    "\n",
    "```\n",
    "I, Q, I, Q, I, Q, I, Q, I, Q, I, Q, I, Q, I, Q, I, Q, I, Q, I, Q, I, Q, I, Q, I, Q, I, Q, ..., I, Q, I, Q, I, Q\n",
    "      ^\n",
    "      Trigger\n",
    "      <-------->\n",
    "         delay\n",
    "                  <--------------------------------- 200 samples -------------------------------------------->\n",
    "\n",
    "             <=================================== 200 samples ==========================================>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
